<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notes for The Final Industry</title>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <a href=".."><- Back</a>
  <h1>Principles of Deep Learning Without Tears</h1>

  <blockquote>Ludwig Boltzmann, who spent much of his life studying statistical mechanics, died in 1906, by his own hand. Paul Ehrenfest, carrying on the work, died similarly in 1933. Now it is our turn to study statistical mechanics.
  
  - David L. Goodstein, States of Matter</blockquote>

  <p>This is going to be a set of notes about the book "The Principles of Deep Learning Theory" (PDLT) which you can follow along in <a href="https://arxiv.org/abs/2106.10165">preprint form</a> or you can buy a digital or real copy from the authors if you're feeling charitable!</p>

  <p>PDLT is written by theoretical physicists working in Statistical Physics to provide a rigorous approach to some foundational questions around classic deep neural network (that is multilayer perceptrons). In writing a "without tears" guide I'm providing a high-level summary of pragmatic results for myself and also cutting through some pretty intense mathematics to communicate a few core concepts of analysis once again for practical leverage to people who don't have a decade to waste studying mathematics or theoretical physics. The book can really be split into two halves, the first half is concerned with "statics" that is given an arbitrary DNN with a distribution on its parameters what can we say about the statistics of its activations as we feed it data and then second is naturally concerned with "dynamics" that is, what happens to these statistics as we train networks what can be said as they converge? I'll provide an overview of both</p>

  <h2>Weight Distributions and Criticality</h2>

  <h3>The Models Used</h3>

  The book covers a few different models to communicate its message, this include linear activation models and ordinary deep neural networks all of which are initialized with weights and biases that are distributed according to zero-mean Gaussian variable. Data accordingly is sampled from a Gaussian with unit variance, we're then 

  <h2>Training and the Neural Tangent Kernel</h2>
</body>
</html>
